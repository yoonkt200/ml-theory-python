{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Linear regression's purpose is minimize the RSS(Residual Sum of Squares). But linear regression can't apply on specific situation. For example, If the dependent value($y$) is non-numerical(e.g. binary categorical), regression result must be weird. So, we have to re-define of this problem as solving probability of binary class.  \n",
    "\n",
    "----\n",
    "\n",
    "### Summary\n",
    "- The logistic(sigmoid) function's output range always $ 0< f(x) < 1 $.\n",
    "- It satisfies probability density function's rule.\n",
    "- So, we can use this for binary classification.\n",
    "\n",
    "$$ f(x) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "\n",
    "----\n",
    "### Logistic Regression for Binary Classification\n",
    "\n",
    "- Now we can modeling $ P(y=1|X) $ when $ y = \\{1,0\\} $. \n",
    "- Follow equation is logistic regression's equation.\n",
    "\n",
    "$$ P(X) = \\frac{1}{1+e^{-(b_0 + WX)}} $$\n",
    "\n",
    "----\n",
    "### How to Derive\n",
    "- It derived from the concept of $odds$.\n",
    "- Below equation means(odds means) that if the P(A) is increases, odds(승산) also increases.\n",
    "\n",
    "$$ odds = \\frac{P(A)}{P(A^c)} = \\frac{P(A)}{1-P(A)}$$\n",
    "\n",
    "- And we have below equation. Because we have to solve this problem as probability of specific class. But left equation's range is $ 0 $ to $ 1 $ and right equation's range is $ -\\infty $ to $ \\infty $ \n",
    "\n",
    "$$ P(y=1|X) = b_0 + WX $$\n",
    "\n",
    "- Let's turn this problem into a odds problem.\n",
    "\n",
    "$$ \\frac{P(y=1|X)}{1 - P(y=1|X)} = b_0 + WX $$\n",
    "\n",
    "- But left equation's range is still $ 0 $ to $ \\infty $. So, we have use $log$.\n",
    "\n",
    "$$ ln(\\frac{P(y=1|X)}{1 - P(y=1|X)}) = b_0 + WX $$\n",
    "\n",
    "- Now, the range of both equation is $ -\\infty $ to $ \\infty $ \n",
    "- Below equations are the process of solving equation line by line.\n",
    "\n",
    "$$ \\frac{P(y=1|X)}{1 - P(y=1|X)})= e^{b_0 + WX} $$\n",
    "\n",
    "$$ \\frac{P(y=1|X)}{1 - P(y=1|X)})= e^{b_0 + WX} $$\n",
    "\n",
    "$$ P(y=1|X) = \\frac{e^{b_0 + WX}}{1+e^{b_0 + WX}} $$\n",
    "\n",
    "$$ \\therefore P(y=1|X) = \\frac{1}{1+e^{-(b_0 + WX)}} $$\n",
    "\n",
    "----\n",
    "### Decision Boundary\n",
    "\n",
    "Generally, The decision boundary of binary classification is 0.5. And logistic regression also like this. Now we follow the process of derive decision boundary.\n",
    "\n",
    "- Make below equation to find decision boundary(Criteria that can be classified as 1).\n",
    "\n",
    "$$ P(y=1|X) > P(y=0|X) $$\n",
    "\n",
    "- And, the below equations are the process of solving equation line by line.\n",
    "\n",
    "> - $$ P(y=1|X) > 1 - P(y=1|X) $$\n",
    "> - $$ P(y=1|X) > 0.5 $$\n",
    "> - $$ \\therefore P(y=1|X) = \\frac{1}{1+e^{-(b_0 + WX)}} > 0.5 $$\n",
    "\n",
    "or\n",
    "\n",
    "> - $$ P(y=1|X) > 1 - P(y=1|X) $$\n",
    "> - $$ \\frac{P(y=1|X)}{1-P(y=1|X)} > 1$$\n",
    "> - $$ ln(\\frac{P(y=1|X)}{1-P(y=1|X)}) > 0$$\n",
    "> - $$ \\therefore b_0 + WX > 0 $$\n",
    "\n",
    "----\n",
    "### Logistic Regression with MLE\n",
    "\n",
    "Now, we have to find cost function for gradient descent. Because of logistic model's non-linearity, we can't use deterministic model. Likelihood is also calculated from PDF functions but by calculating the joint probabilities of data points from a particular PDF function. In this case, instead of normal distribution, Bernoulli distribution is our proper PDF.\n",
    "\n",
    "$$ L(parametes | data) = \\prod_{i=1}^{m} f(data_i | parameters) $$\n",
    "\n",
    "- Bernoulli trial : bernoulli trial means that the results of experiment(=event) are binary case.\n",
    "- For example, the binary probability of event is below.\n",
    "\n",
    "$$ P(y=1) = p, P(y=0) = 1-p $$\n",
    "\n",
    "$$ P(Y=y_i) = p^{y_{i}} (1-p)^{1-y_i}, (y=1,0) $$\n",
    "\n",
    "- And Bernoulli distribution is like that.\n",
    "\n",
    "$$ Bern(y; p) = p^{y} (1-p)^{1-y} $$\n",
    "\n",
    "- Now we can summary some equations. ($ Note, f(x)=logistic(x) $)\n",
    "\n",
    "$$ L = \\prod_i p^{y_{i}} (1-p)^{1-y_i}$$\n",
    "\n",
    "$$ L = \\prod_i f(b_0 + XW)^{y_{i}} (1-f(b_0 + XW))^{1-y_i}$$\n",
    "\n",
    "$$ ln(L) = \\sum_i y_i ln(f(b_0 + XW)) + \\sum_i (1-y_i)ln(1-f(b_0 + XW)))$$\n",
    "\n",
    "- So, the log-likelihood of logistic regression model is defined. And this is cost function of logistic regression.\n",
    "\n",
    "----\n",
    "### Gradient Descent\n",
    "\n",
    "- After long long long long formula expansion [(link)](https://stats.stackexchange.com/questions/278771/how-is-the-cost-function-from-logistic-regression-derivated), we can find the gradient of cost function (log-likelihood)  \n",
    "\n",
    "$$ \\frac{d}{dW}ln(L) = \\frac{1}{m} \\sum_{i=1}^{m}(f(WX) - y_i)x_j $$\n",
    "\n",
    "$$ w_j = w_j - \\frac{\\alpha}{m} \\sum_{i=1}^{m}(f(WX) - y_i)x_j $$\n",
    "\n",
    "----\n",
    "#### references\n",
    "- https://machinelearningmastery.com/logistic-regression-for-machine-learning/\n",
    "- https://ratsgo.github.io/machine%20learning/2017/04/02/logistic/\n",
    "- https://stats.stackexchange.com/questions/278771/how-is-the-cost-function-from-logistic-regression-derivated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
